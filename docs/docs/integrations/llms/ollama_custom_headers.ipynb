{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a931120",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This notebook demonstrates how to use the Ollama integration in LangChain with support for passing custom HTTP headers.  \n",
    "This is useful for scenarios where authentication or custom tracking headers are required by your Ollama model endpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce72f0f0",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Make sure you have the `langchain-ollama` package installed and your Ollama endpoint accessible.  \n",
    "You may need to set environment variables or have your API keys ready if your Ollama server requires authentication.\n",
    "\n",
    "```bash\n",
    "pip install langchain-ollama\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282d006b",
   "metadata": {},
   "source": [
    "## Instantiation\n",
    "\n",
    "Below is how to instantiate the `ChatOllama` model with custom headers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba41e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "headers = {\n",
    "    \"Authorization\": \"Bearer YOUR_ACCESS_TOKEN\",\n",
    "    \"X-Custom-Header\": \"LangChainIntegrationTest\",\n",
    "}\n",
    "\n",
    "llm = ChatOllama(\n",
    "    base_url=\"http://your-ollama-server:11434\",\n",
    "    model=\"llama3-groq-tool-use\",\n",
    "    headers=headers,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca8843c",
   "metadata": {},
   "source": [
    "## Invocation\n",
    "\n",
    "Invoke the model with chat messages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cae50ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import ChatMessage\n",
    "\n",
    "messages = [ChatMessage(role=\"user\", content=\"Hello Ollama with custom headers!\")]\n",
    "\n",
    "response = llm.invoke(messages)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac18797",
   "metadata": {},
   "source": [
    "## Chaining\n",
    "\n",
    "You can also use this model inside LangChain chains, for example with `LLMChain`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32989038",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate(template=\"Say hi to {name}!\", input_variables=[\"name\"])\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "result = chain.run({\"name\": \"LangChain User\"})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d19f82",
   "metadata": {},
   "source": [
    "## API reference\n",
    "\n",
    "- `ChatOllama.__init__(base_url: str, model: str, headers: Optional[dict] = None, ...)`  \n",
    "    Initialize with optional custom HTTP headers.\n",
    "\n",
    "- `invoke(messages: List[ChatMessage])`  \n",
    "    Sends chat messages to the Ollama model endpoint and returns the response.\n",
    "\n",
    "- See full API docs: [LangChain Ollama integration](https://python.langchain.com/en/latest/modules/llms/integrations/ollama.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
